{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# 02: 分词器对比（fugashi/MeCab, SudachiPy, SentencePiece）\nimport sys, os\nrepo_root = os.path.abspath(\".\")\nif repo_root not in sys.path: sys.path.append(repo_root)\n\n# 确保依赖\n!pip -q install fugashi unidic-lite SudachiPy sudachidict-core sentencepiece > /dev/null\n\nfrom fugashi import Tagger\nfrom sudachipy import dictionary, tokenizer as tknz\nfrom transformers import AutoTokenizer\nfrom src.data.load_jglue import load_jnli\nimport numpy as np, pandas as pd, random\n\ntagger = Tagger()\nsud = dictionary.Dictionary().create()\nspm = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt-neox-3.6b\")\n\ndef mecab_tokens(t): return [m.surface for m in tagger(t)]\ndef sudachi_tokens(t, mode=tknz.Tokenizer.SplitMode.C): return [m.surface() for m in sud.tokenize(t, mode)]\ndef spm_tokens(t): return spm.convert_ids_to_tokens(spm(t, add_special_tokens=False)[\"input_ids\"])\n\n# 样例对比\nsample_text = \"本日は晴天なり。生成モデルの分かち書きとサブワード分割を比較します。2024年のデータ。\"\nprint(\"TEXT:\", sample_text)\nprint(\"MeCab:\", mecab_tokens(sample_text))\nprint(\"Sudachi(C):\", sudachi_tokens(sample_text))\nprint(\"SPM:\", spm_tokens(sample_text))\n\n# 500 例统计\nds = load_jnli(\"validation\")\nsents = [x[\"sentence1\"] for x in ds.select(range(min(500, len(ds))))]\n\ndef avg_len(fn):\n    lens = [len(fn(s)) for s in sents]\n    return float(np.mean(lens)), float(np.percentile(lens,95))\n\nres = pd.DataFrame([{\n    \"tokenizer\":\"MeCab\", \"mean_len\": avg_len(mecab_tokens)[0], \"p95_len\": avg_len(mecab_tokens)[1]\n},{\n    \"tokenizer\":\"Sudachi(C)\", \"mean_len\": avg_len(sudachi_tokens)[0], \"p95_len\": avg_len(sudachi_tokens)[1]\n},{\n    \"tokenizer\":\"SentencePiece\", \"mean_len\": avg_len(spm_tokens)[0], \"p95_len\": avg_len(spm_tokens)[1]\n}])\nprint(res)\n\n# 保存 CSV\nimport pathlib\nout = pathlib.Path(\"reports/phase1\"); out.mkdir(parents=True, exist_ok=True)\nres.to_csv(out/\"tokenizer_len_stats.csv\", index=False)\nprint(\"Saved:\", out/\"tokenizer_len_stats.csv\")\n",
      "outputs": [],
      "execution_count": null
    }
  ]
}